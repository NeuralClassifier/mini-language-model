# Embedding

Once the Vocabulary Map is created, each token $t_i$ is mapped to an embedding matrix $E$:

$$
E = T^x \to R^d
$$

where, $d$ is the embedding dimension, and embedding $E(t_i)$ for each token $t_i$ is:

$$
e_i = E(t_i) \in R^d
$$