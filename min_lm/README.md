# MiniLM Model

Embedding Layer: Converts input token indices into dense vector representations.

Positional Encoding: Adds information about the position of words in the sequence.

Transformer Layer: Processes the embeddings using self-attention mechanisms.

Output Projection: Maps the processed embeddings to the vocabulary space.
